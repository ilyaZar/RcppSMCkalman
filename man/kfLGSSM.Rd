% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/01_kf_FFBS.R
\name{kfLGSSM}
\alias{kfLGSSM}
\title{Kalman forward filtering, prediction and smoothing.}
\usage{
kfLGSSM(
  yObs,
  uReg = NULL,
  wReg = NULL,
  A = NULL,
  B = NULL,
  C = NULL,
  D = NULL,
  Q = NULL,
  R = NULL,
  initX = NULL,
  initP = NULL,
  initU = NULL,
  nSimMF = 1,
  nSimPD = 1,
  nSimMS = 1,
  nSimJS = 1,
  computeMFD = TRUE,
  computePRD = TRUE,
  computeMSD = TRUE,
  computeJSD = TRUE,
  computeLLH = TRUE
)
}
\arguments{
\item{yObs}{A matrix or vector of measurements (observations):
 \itemize{
 \item{rows: multivariate dimension}
 \item{columns: time series dimension \code{T}}
 }
If \code{Y} is a univariate process, \code{yObs} can be passed as a vector
of length \code{T}. If \code{nrow(yObs) = 1}, then \code{yObs} becomes a
vector of length \code{T}. This argument can not be missing.}

\item{uReg}{Matrix (vector) of regressors for the latent state process of
dimension \code{ncol(B) x T}. For a single regressors \code{uReg} is a
vector of length \code{T}.}

\item{wReg}{Matrix (vector) of regressors for the measurement process of
dimension \code{ncol(D) x T}. For a single regressors \code{wReg} is a
vector of length \code{T}.}

\item{A}{Parameter (or system) matrix of dimension \code{dimX x dimX}.}

\item{B}{Parameter (or system) matrix of dimension \code{dimX x numU}.}

\item{C}{Parameter (or system) matrix of dimension \code{dimY x dimX}.}

\item{D}{Parameter (or system) matrix of dimension \code{dimY x numW}.}

\item{Q}{VCM of state process of dimension \code{dimX x dimX}.}

\item{R}{VCM of measurement process of dimension \code{dimY x dimY}}

\item{initX}{Initial value for state process. Think of \eqn{X_{t=0}} as a
starting/initial condition for a time series running for \code{t=1,...,T}.
If not specified, then \code{initX} is sampled from the prior under the
stationary distribution of the latent state process (see \code{Details}).}

\item{initP}{VCM matrix initialization for state process; if not specified,
then the prior covariance-matrix under the stationary distribution of the
latent states is used (see \code{Details}).}

\item{initU}{Initial values of regressors of the state process specification
(cannot be missing).}

\item{nSimMF}{Number of forward filtering runs; defaults to \code{nSimMF=1}.}

\item{nSimPD}{Number of prediction runs; defaults to \code{nSimPD=1}.}

\item{nSimMS}{Number of marginal smoothing runs; defaults to \code{nSimMS=1}.}

\item{nSimJS}{Number of joint smoothing (backward simulation) runs; defaults
to \code{nSimJS=1}.}

\item{computeMFD}{Logical: if \code{TRUE}, then the marginal filtering
density (means and variances) and \code{nSimMF} forward
filtering runs are computed and returned.}

\item{computePRD}{Logical: if \code{TRUE}, then the prediction density
(means and variances) and \code{nSimPD} predictions are}

\item{computeMSD}{Logical: if \code{TRUE}, then the marginal smoothing
density (means and variances) and \code{nSimMS} marginal
smoothing runs are computed and returned.}

\item{computeJSD}{Logical: if \code{TRUE}, then the joint smoothing density
(means and variances) and \code{nSimJS} joint smoothing (i.e
backward simulation) runs are computed and returned.}

\item{computeLLH}{Logical: if \code{TRUE}, then the log-likelihood (data
density) is returned.}
}
\value{
A named list of 4:
  \itemize{
    \item{\code{kfMarginalFilteringDensity:} list of length \code{nSimMF}
    with each element being one complete marginal filtering series.}
    \item{\code{kfVCMmfd:} the marginal filtering VCM estimates i.e. the
    \eqn{P_{t|t}} matrices, see \code{Details}.}
    \item{\code{kfJointSmoothingDensity:} list of length \code{nSimJS} with
    each element being one complete joint smoothing density pass.}
    \item{\code{kfVCMjsd:} the joint smoothing VCM estimates i.e. the
    \eqn{J_{t|t}} matrices, i.e. see \code{Details}.}
  }
}
\description{
Under a Linear Gaussian State Space Model (LGSSM) specification, the function
runs an (exact) Kalman forward (marginal) filter, obtains predictions,
computes marginal and joint smoothing densities (the former sometimes
referred to as the RST-smoother), and generates an unbiased estimate of the
(log-)likelihood. \code{computeXXX}-flags control the amount of computation
(e.g. only a forward filter and log-likelihood computations, but skipping
prediction the smoothers), while \code{nSimXX} arguments set the number of
simulation from above densities (if required).
}
\details{
\subsection{Model specification}{

The specification of the LGSSM is of the following form:
\deqn{x_t = Ax_{t-1} + Bu_t + v_t \\}
\deqn{y_t = Cx_t + Dw_t + \varepsilon_t\;,}
with serially uncorrelated state innovations \eqn{v_t\sim\mathcal{N}(0, Q)}
and measurement errors \eqn{\varepsilon_t\sim\mathcal{N}(0,R)}, for
\eqn{t=1,\ldots,T}. The argument names match the previous equations. If an
argument is missing, the corresponding component is dropped (i.e. set to
zero), and forward filtering, prediction and backward smoothing are run under
this specification as usual.

There can be an initial state (vector) value passed via \code{initX} as well
as an initial "prediction" or VCM matrix at period \eqn{t=0} via \code{initP}
for initialization of the algorithm. If these are not provided, the
stationary prior is used as a default initializer having the following form:
\deqn{x_0\sim\mathcal{N}\left(Bu_0(I-A)^{-1},\left(I-A\right)^{-1}Q
\left[\left(I-A\right)^{-1}\right]^{\top}\right)\;,} where \eqn{I} is the
identity matrix, and \eqn{u_0} is \code{initU}. In the univariate case
(\code{length(initX) = 1}), the prior reduces to
\eqn{x_0\sim\mathcal{N}\left(\frac{B\cdot u_0}{1-A},
\frac{Q}{(1-A^2)}\right)}, and \eqn{B\cdot u_0} is the "dot" or scalar
product if the corresponding number of \code{u}-type regressors \code{>1}.

Note that above specification requires an initial regressor (vector) value
\code{initU} if \code{initX} is missing, but can be dropped if \code{initX}
is provided.
}

\subsection{(Marginal) Filtering and prediciton - implemented recursions}{
\enumerate{
  \item {\code{For }\eqn{t=0:} \code{compute} \eqn{\hat{x}_{0|0}}
  and \eqn{\hat{x}_{0|0}} \code{according to:}
    \deqn{\hat{x}_{0|0} = Bu_0\left(I-A\right)^{-1}}
    \deqn{\hat{P}_{0|0} =\left(I-A\right)^{-1}Q
    \left[\left(I-A\right)^{-1}\right]^{\top}}
  }
  \item {\code{For }\eqn{t=1,\ldots,T:} \code{compute:}
    \itemize{
      \item \eqn{K_t} and \eqn{L_t}
      \deqn{K_t = \hat{P}_{t|t-1} C^{\top} = \left(A\hat{P}_{t-1|t-1}
       A^{\top} + Q\right) C^{\top}}
      \deqn{L_t = \left[C\hat{P}_{t|t-1} C^{\top} + R\right]^{-1} =
      \left[C \left(A\hat{P}_{t-1|t-1} A^{\top} + Q\right) C^{\top} +
      R\right]^{-1}}
      \item \code{for marginal filtering:}
       \deqn{\hat{x}_{t|t} = \hat{x}_{t|t-1} + K_tL_t
       \left(y_t - C\hat{x}_{t|t-1}-Dw_t\right) = A\hat{x}_{t-1|t-1} +
       Bu_{t-1} + K_tL_t\left(y_t -C\left[A\hat{x}_{t-1|t-1} + Bu_{t-1}
       \right]-Dw_t\right)}
       \deqn{\hat{P}_{t|t} = \hat{P}_{t|t-1} - K_t L_t K_t^{\top} =
       A\hat{P}_{t-1|t-1} A^{\top} + Q - K_t L_t K_t^{\top}}
      \item \code{for prediction:}
      \deqn{\hat{P}_{t|t-1} = A\hat{P}_{t-1|t-1} A^{\top} + Q}
      \deqn{\hat{x}_{t|t-1} = A\hat{x}_{t-1|t-1} + Bu_{t-1}}
      }
  }
}
}
}
